{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>capital</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>2174</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>284582</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>160187</td>\n",
       "      <td>9th</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>16</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>209642</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31</td>\n",
       "      <td>Private</td>\n",
       "      <td>45781</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>14084</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42</td>\n",
       "      <td>Private</td>\n",
       "      <td>159449</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>5178</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education         marital-status  \\\n",
       "0   39         State-gov   77516  Bachelors          Never-married   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors     Married-civ-spouse   \n",
       "2   38           Private  215646    HS-grad               Divorced   \n",
       "3   53           Private  234721       11th     Married-civ-spouse   \n",
       "4   28           Private  338409  Bachelors     Married-civ-spouse   \n",
       "5   37           Private  284582    Masters     Married-civ-spouse   \n",
       "6   49           Private  160187        9th  Married-spouse-absent   \n",
       "7   52  Self-emp-not-inc  209642    HS-grad     Married-civ-spouse   \n",
       "8   31           Private   45781    Masters          Never-married   \n",
       "9   42           Private  159449  Bachelors     Married-civ-spouse   \n",
       "\n",
       "          occupation   relationship   race     sex  hours-per-week  \\\n",
       "0       Adm-clerical  Not-in-family  White    Male              40   \n",
       "1    Exec-managerial        Husband  White    Male              13   \n",
       "2  Handlers-cleaners  Not-in-family  White    Male              40   \n",
       "3  Handlers-cleaners        Husband  Black    Male              40   \n",
       "4     Prof-specialty           Wife  Black  Female              40   \n",
       "5    Exec-managerial           Wife  White  Female              40   \n",
       "6      Other-service  Not-in-family  Black  Female              16   \n",
       "7    Exec-managerial        Husband  White    Male              45   \n",
       "8     Prof-specialty  Not-in-family  White  Female              50   \n",
       "9    Exec-managerial        Husband  White    Male              40   \n",
       "\n",
       "  native-country  capital income  \n",
       "0  United-States     2174  <=50K  \n",
       "1  United-States        0  <=50K  \n",
       "2  United-States        0  <=50K  \n",
       "3  United-States        0  <=50K  \n",
       "4           Cuba        0  <=50K  \n",
       "5  United-States        0  <=50K  \n",
       "6        Jamaica        0  <=50K  \n",
       "7  United-States        0   >50K  \n",
       "8  United-States    14084   >50K  \n",
       "9  United-States     5178   >50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Import supplementary visualization code visuals.py\n",
    "import visuals as vs\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "# Load the Census dataset\n",
    "data = pd.read_csv(\"us census data.csv\")\n",
    "# Success - Display the first record\n",
    "display(data.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48842, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of records\n",
    "n_records = len(data)\n",
    "\n",
    "# Number of records where individual's income is more than $50,000\n",
    "n_greater_50k = len(data[data['income']=='>50K'])\n",
    "\n",
    "# Number of records where individual's income is at most $50,000\n",
    "n_at_most_50k = len(data[data['income']=='<=50K'])\n",
    "\n",
    "# Percentage of individuals whose income is more than $50,000\n",
    "greater_percent = (n_greater_50k/n_records)*100\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of records: {}\".format(n_records))\n",
    "print(\"Individuals making more than $50,000: {}\".format(n_greater_50k))\n",
    "print(\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\n",
    "print(\"Percentage of individuals making more than $50,000: {}%\".format(greater_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.replace({'?':np.nan})\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['capital-gain']=[x if int(x)>0 else 0 for x in data['capital']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['capital-loss']=[abs(x) if int(x)<0 else 0 for x in data['capital']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['capital'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['fnlwgt'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['education-num']=data['education'].replace({'Bachelors':13.0,'HS-grad':9.0,'11th':7.0,'Masters':14.0,'9th':5.0,\n",
    "                                               'Some-college':10.0,'Assoc-acdm':12.0,'7th-8th':4.0,'Doctorate':16.0,\n",
    "                                              'Assoc-voc':12.0,'Prof-school':15.0,'5th-6th':3.0,'10th':6.0,'Preschool':1.0,\n",
    "                                              '12th':8.0,'1st-4th':2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target label\n",
    "income_raw = data['income']\n",
    "features_raw = data.drop('income', axis = 1)\n",
    "# Visualize skewed continuous features of original data\n",
    "vs.distribution(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the skewed features\n",
    "skewed = ['capital-gain', 'capital-loss']\n",
    "features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n",
    "# Visualize the new log distributions\n",
    "vs.distribution(features_log_transformed, transformed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'income_raw' data to numerical values\n",
    "income_raw.replace('<=50K', 0, inplace=True)\n",
    "income_raw.replace('>50K', 1, inplace=True)\n",
    "income=income_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "cat_features=['workclass', 'education','marital-status', 'occupation','relationship', 'race', 'sex','native-country']\n",
    "features_log_minmax_transform = pd.DataFrame(data = features_log_transformed)\n",
    "features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n",
    "\n",
    "# Show an example of a record with scaling applied\n",
    "display(features_log_minmax_transform.head(n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Final Datasets\n",
    "Here we generate all the needed datasets and save them in a dictionary. One dataset is that all the categorical variables are one-hot encoding.  For the other dataset,  we first bin the native-country variable into two categories and then one-hot encoding all the categorical variables. Finally, we generate a dataset where all the categorical variables without encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final={}\n",
    "features_final = pd.get_dummies(features_log_minmax_transform)\n",
    "data_final['native_country_one_hot']=features_final \n",
    "data_final['catboost_data']=features_log_minmax_transform.copy()\n",
    "features_log_minmax_transform['new_native-country']=[x if x=='United-States' else 'other' for x in features_log_minmax_transform['native-country']]\n",
    "features_log_minmax_transform.drop(['native-country'],inplace=True,axis=1)\n",
    "native_country_bin = pd.get_dummies(features_log_minmax_transform)\n",
    "data_final['native_country_bin']=native_country_bin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Training and Predicting Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import two metrics from sklearn - fbeta_score and accuracy_score\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "def train_predict(learner,features_final): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - features_final: the final dataset      \n",
    "    '''\n",
    "    # Split the 'features' and 'income' data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_final, \n",
    "                                                        income, \n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 0)   \n",
    "    results = {}\n",
    "    # Fit the learner to the training data \n",
    "    start = time() # Get start time\n",
    "    learner = learner.fit(X_train, y_train)\n",
    "    end = time() # Get end time  \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end-start       \n",
    "    # Get the predictions on the test set(X_test),\n",
    "    #       then get predictions on the training samples(X_train) using .predict()\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() # Get end time\n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end-start       \n",
    "    # Compute accuracy on the training samples\n",
    "    results['acc_train'] = accuracy_score(y_train,predictions_train)   \n",
    "    # Compute accuracy on test set using accuracy_score()\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    # Compute F-score on the training samples using fbeta_score()\n",
    "    results['f_train'] =fbeta_score(y_train,predictions_train, beta=0.5)   \n",
    "    # Compute F-score on the test set which is y_test\n",
    "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=0.5)  \n",
    "    # Success\n",
    "    print(\"{} trained.\".format(learner.__class__.__name__)) \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of GradientBoostingClassifier and CatBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the two models\n",
    "clf_A = GradientBoostingClassifier(n_estimators=500)\n",
    "clf_B = CatBoostClassifier(verbose=0, n_estimators=500,cat_features=cat_features,one_hot_max_size=2)\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    if clf==clf_A:\n",
    "        results[clf_name]=train_predict(clf,data_final['native_country_one_hot'])\n",
    "    else:\n",
    "        results[clf_name]=train_predict(clf,data_final['catboost_data'])\n",
    "# Run metrics visualization for the two models\n",
    "vs.evaluate(results,'Performance Metrics for GradientBoostingClassifier and CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- First, as we see, CatBoostClassifier has a much lower train and predict time than the GradientBoostingClassifier. This is because the two classifiers use different encoding methods for categorical variables. For GradientBoostingClassifier, we use One-Hot Encoding which increases the dimensionality of the features significantly if there are high cardinality variables. For CatBoostClassifier, we use the built-in default setting of transforming categorical features to numerical features.  One-Hot Encoding is only used for the variables with not more than two categories, otherwise, the Borders method is used. As a result, the computation complexity is much lower and then a much lower computation time. \n",
    "- Second, the performance of the two classifiers are almost the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of GradientBoostingClassifier with Different Encoding Methods for the Variable native-country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "clf = GradientBoostingClassifier(n_estimators=500)\n",
    "# Collect results on the datasets\n",
    "results = {}\n",
    "del data_final['catboost_data']\n",
    "for k, v in data_final.items():\n",
    "    data_name =k \n",
    "    results[data_name] = {}    \n",
    "    results[data_name]=train_predict(clf,v)    \n",
    "# Run metrics visualization on the two dataset\n",
    "vs.evaluate(results,'Performance Metrics for GradientBoostingClassifier with Different Encoding Methods for the Variable native-country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_one_hot = list(data_final['native_country_one_hot'].columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded_one_hot)))\n",
    "encoded_native_country_bin_one_hot = list(data_final['native_country_bin'].columns)\n",
    "print(\"{} total features after native-country bin and one-hot encoding.\".format(len(encoded_native_country_bin_one_hot)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- First, as we see, native_country_bin has a much lower train and predict time than the native_country_one_hot. This is because, for native_country_bin, we reduce forty categories to only two categories where we merge all the categories other than 'United States' to one bin before one-hot encoding. As a result, the computation complexity is reduced.\n",
    "- Second, the performance of native_country_bin is a little higher than the native_country_one_hot. This is because we merge all the categories other than 'United States' to one bin where we remove some information. There is an effect of reducing overfitting. But it does not mean we will always get a higher performance this way. On the other hand, we also removed some information. So the performance will depend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
